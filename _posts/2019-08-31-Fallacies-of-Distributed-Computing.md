---
layout: post
title:  "Fallacies of Distributed Computing (Part 1)"
date:   2019-08-31 18:53:08 +0200
categories: distributed systems
---

The [fallacies of distributed computing](https://de.wikipedia.org/wiki/Fallacies_of_Distributed_Computing) is a set of assumptions that programmers make when designing or managing distributed systems. In this post, I plan to jot down, how some existing distributed systems' designs, consider handling the fallacies of distributed computing:

**Network is reliable**: Network calls can fail. Hardware and software, both can fail: power supplies, routers, failed patches etc. Most applications make use of an external service or communicate with another micro-service using an API, and these calls will fail. This inevitable problem is solved reduced by taking measures on the client and the server-side. Client reinforce retries on calls to a service (with a timeout on response) and servers maintain redundancy over network. AWS provides 3 different Availability Zones (data centers) within a single region for applications to be redundant, and keep replying even if an AZ goes down or loses network communication completely.  

**Latency is zero**: Calls over a network are not instant. There are different patterns to be utilized based on the requirements of the system. Network communication should not happen too often, it makes any synchronous task that the calling system does wait a lot for the replies. A call can even hang up, and having a timeout on time-critical requests is a way to realize something is wrong. The issue of latency gets more prominent with the problem of synchronization in distributed systems. For example, Apache Zookeeper is used to implement distributed locks. If an instance A acquires a lock on some data, does a prolonged task, while another system B is waiting for the lock, B should fail and retry if necessary, informing the cause (by logging, for instance) to the application developer.

**Bandwidth is infinite**: It is important to realize bandwidth of systems is not unlimited. Bandwidth is the capacity of a network to send data over a period of time. 

For a simple example, in a streaming system like [Nakadi](https://nakadi.io) (or Apache Kafka), a lot of producers and consumers are connected to the system, publishing and consuming gigabytes of data per second. In such a system, if the instances are reaching their bandwidth limit, the problem is solved by the distribution of load over a number of instances (upscaling). If however, the network is reaching bandwidth limits, the answer is partitioning domain to bounded context. For example, for a system like Nakadi, the solution might come in as having different networks serving inbound/outbound traffic. AWS lists the different bandwidth (and throttling) limits for all it's instance types, and right instance-type for an application needs to be decided based on a number of different factors including the bandwidth an application instance would use. 

Also, solution to second fallacy competes with this one, as there should be a bound on how much data to fetch in a single call to reduce latency based on the bandwidth of the system. 

**Network is secure**: The insecurity of networks influences the design and architecture of systems a lot. One such [design](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html) that I've seen is an application where there are some parts of an application that needs no connection to the Internet (private subnet), parts that need to access the Internet, but should not be reachable from the outside (private subnets, access to the Internet through NAT hosted in public subnet) and parts that need connection both ways (public subnets).  Not only on the network level, but on the application level, a lot of applications handle authentication and authorization at different levels. For example, Kafka, Zookeeper, and many other applications implement authorization on application level.

**Topology doesn't change**: Instances can die, and the topology of a distributed system can change too much too fast. Different instances of a same application can perform different roles, for example, in most systems, there is a leader and many followers. However, an instance handling a certain role may die, and their role should be picked to be performed by another instance of the same application. For handling this, distributed key value stores, with both ephemeral and permanent structures are used. For example, one of such system which is widely used is Apache Zookeeper, which is a fast & distributed hierarchical data file system and is used for handling group membership, sharing distributed data/configuration, and implementing distributed data structures like locks, queues, etc.

This is a very simple post discussing the simple examples of fallacies of distributed systems, and what some systems intend to solve with the features they offer. I'll create a second post for the 3 remaining fallacies of distributed computing and keep adding to this one with more solutions that existing systems offer.
